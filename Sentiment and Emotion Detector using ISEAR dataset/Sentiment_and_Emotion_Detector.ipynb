{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Building an Emotion Classifier Model"
      ],
      "metadata": {
        "id": "5avqHIhDEjOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting Data"
      ],
      "metadata": {
        "id": "hcHPIZhWaMKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur482SwREnJQ",
        "outputId": "3d81bf91-4a24-4560-fda8-66fd9fdf813e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv('drive/My Drive/Data/ISEAR/data_train.csv')\n",
        "test_data = pd.read_csv('drive/My Drive/Data/ISEAR/data_test.csv')"
      ],
      "metadata": {
        "id": "NeXd8pjgaRFk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.columns = [\"Emotion\",\"Text\"]\n",
        "test_data.columns = [\"Emotion\",\"Text\"]\n",
        "print(train_data.head())\n",
        "print(test_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgP7YEyJa7JO",
        "outputId": "42377a85-c1b5-4992-8ad0-5aad5267348f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Emotion                                               Text\n",
            "0  neutral   There are tons of other paintings that I thin...\n",
            "1  sadness  Yet the dog had grown old and less capable , a...\n",
            "2     fear  When I get into the tube or the train without ...\n",
            "3     fear  This last may be a source of considerable disq...\n",
            "4    anger  She disliked the intimacy he showed towards so...\n",
            "   Emotion                                               Text\n",
            "0  sadness  I experienced this emotion when my grandfather...\n",
            "1  neutral   when I first moved in , I walked everywhere ....\n",
            "2    anger  ` Oh ! \" she bleated , her voice high and rath...\n",
            "3     fear  However , does the right hon. Gentleman recogn...\n",
            "4  sadness  My boyfriend didn't turn up after promising th...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning and Preprocessing"
      ],
      "metadata": {
        "id": "v2aj8iFRbaDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "KoVXJhhTdVzT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "  #make the text lower case\n",
        "  data['Text'] = data['Text'].apply(lambda a: \" \".join(a.lower() for a in a.split()))\n",
        "  #remove non-word characters (^\\w) or white space characters (\\s)\n",
        "  data['Text'] = data['Text'].apply(lambda a: \" \".join(a.replace('[^\\w\\s]','') for a in a.split()))\n",
        "  #remove stop words\n",
        "  stop = stopwords.words('english')\n",
        "  data['Text'] = data['Text'].apply(lambda a: \" \".join(a for a in a.split() if a not in stop))\n",
        "  #correct spelling\n",
        "  data['Text'] = data['Text'].apply(lambda a: str(TextBlob(a).correct()))\n",
        "  #do stemming\n",
        "  st = PorterStemmer()\n",
        "  data['Text'] =  data['Text'].apply(lambda a: \" \".join([st.stem(word) for word in a.split()]))\n",
        "  return data\n",
        "\n",
        "\n",
        "train_data = preprocess_data(train_data)\n",
        "test_data = preprocess_data(test_data)\n",
        "print(train_data.head())\n",
        "print(test_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Xb8spkxZbbUt",
        "outputId": "a02d10f3-7361-407c-f699-e5b7987a0c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Emotion                                               Text\n",
              "0  neutral                           ton paint think better .\n",
              "1  sadness  yet dog grown old less capabl , one day gilli ...\n",
              "2     fear                 get tube train without pay ticket.\n",
              "3     fear  last may sourc consider disquiet one might fir...\n",
              "4    anger  dislik intimaci show toward , resent memori sh..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4ff94784-005d-4bca-9c9a-57010154a70c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>ton paint think better .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sadness</td>\n",
              "      <td>yet dog grown old less capabl , one day gilli ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fear</td>\n",
              "      <td>get tube train without pay ticket.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fear</td>\n",
              "      <td>last may sourc consider disquiet one might fir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>anger</td>\n",
              "      <td>dislik intimaci show toward , resent memori sh...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ff94784-005d-4bca-9c9a-57010154a70c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4ff94784-005d-4bca-9c9a-57010154a70c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4ff94784-005d-4bca-9c9a-57010154a70c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to_csv(\"drive/My Drive/Data/ISEAR/pre_processsed_data_train.csv\")\n",
        "test_data.to_csv(\"drive/My Drive/Data/ISEAR/pre_processsed_data_test.csv\")"
      ],
      "metadata": {
        "id": "PfLY5v58hcQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv('drive/My Drive/Data/ISEAR/pre_processsed_data_train.csv')\n",
        "test_data = pd.read_csv('drive/My Drive/Data/ISEAR/pre_processsed_data_test.csv')"
      ],
      "metadata": {
        "id": "wXLLRfNqe2tB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Sience :\n",
        "\n",
        "Build vocabulary, vectorize and Data set and classifier classes"
      ],
      "metadata": {
        "id": "OAnDbKXNgVBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "J0VGMZh7gUfb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the main vocabulary class job : creates a token to index and a index to token dictionary. we can access to one of each, given the other\n",
        "class Vocabulary(object):\n",
        "  def __init__(self,data=None,add_unk = True,unk_token = \"<UNK>\",add_mask = True,mask_token=\"<MASK>\"):\n",
        "    self.token_to_index = {}\n",
        "    self.index_to_token = {}\n",
        "    #mask and unk tokens are optional depending on the vocabulary and the problem\n",
        "    self.add_unk = add_unk\n",
        "    if add_unk :\n",
        "      self.unk_token = unk_token\n",
        "      self.add_token(unk_token)\n",
        "      self.unk_index = self.token_to_index[unk_token]\n",
        "    if add_mask:\n",
        "      self.add_token(mask_token)\n",
        "      self.mask_index = self.token_to_index[mask_token]\n",
        "    if data != None :\n",
        "      for row in data :\n",
        "        for token in row.split():\n",
        "          self.add_token(token)\n",
        "\n",
        "  #this function gets one token and add it to the dictionary by updating both token_to_index and index_to_token      \n",
        "  def add_token(self,token):\n",
        "    if token not in self.token_to_index :\n",
        "      next_index_in_vocab = len(self.token_to_index)\n",
        "      self.token_to_index[token] = next_index_in_vocab\n",
        "      self.index_to_token[next_index_in_vocab] = token\n",
        "  # this function search for a token and returns its corresponding token, if the token is not in the vocabulary and vocabulary supports \n",
        "  # unk tokens it returns the index of unk token, otherwise it raise an error \n",
        "  def lookup_token(self,token):\n",
        "    if self.add_unk:\n",
        "      return self.token_to_index.get(token,self.unk_index)\n",
        "    else :\n",
        "      return self.token_to_index[token]\n",
        "  # this function search for a index and returns its corresponding token\n",
        "  def lookup_index(self,index):\n",
        "    return self.index_to_token[index]\n",
        "\n",
        "  # returns the legth of the vocabulary\n",
        "  def __len__(self):\n",
        "    return len(self.token_to_index)\n",
        "\n",
        "  def use_previous_token_to_index(self,token_to_index):\n",
        "    self._token_to_idx = token_to_index\n",
        "    self.index_to_token = {idx: token for token, idx in self.token_to_index.items()}"
      ],
      "metadata": {
        "id": "UsVKTqjpiAtq"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the main job of Vectorizer class: it is responsible for converting a text (sequence of tokens) to a vectorized version of it (sequence of indexes)\n",
        "#so it can be used by neural network layers\n",
        "\n",
        "class Vectorizer(object):\n",
        "  def __init__(self,text_vocab,emotion_vocab):\n",
        "    self.text_vocab = text_vocab\n",
        "    self.emotion_vocab = emotion_vocab\n",
        "  #vector_length is usually the lentgh of the maximum text\n",
        "  #although we have textes with different lengthes but we neet  put them in a fixed-size vector and fill the remaining of the vector with mask\n",
        "  #tokens. I also return the actual length of each text\n",
        "  def vectorize(self,text,vector_length):\n",
        "    indices = [self.text_vocab.lookup_token(token) for token in text.split(' ')]\n",
        "    out_vector = np.zeros(vector_length, dtype=np.int64)\n",
        "    out_vector[:len(indices)] = indices\n",
        "    out_vector[len(indices):] = self.text_vocab.mask_index\n",
        "    return out_vector,len(indices)\n"
      ],
      "metadata": {
        "id": "1fh_XyKRnG9R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The main job of Dataset class : Dataset class inherits from Dataset class in pytorch and implements two essential funtions of it, __getitem()__\n",
        "#and __len__(), this class in being used for getting the dataset rows (vectorized version) during training and testing \n",
        "class Dataset(Dataset):\n",
        "  def __init__(self,dataframe,vectorizer):\n",
        "    self.dataframe = dataframe\n",
        "    self.vectorizer = vectorizer\n",
        "    measure_len = lambda text: len(text.split(\" \"))\n",
        "    self._max_text_length = max(map(measure_len, dataframe.Text))\n",
        "\n",
        "    # Class weights\n",
        "    class_counts = self.dataframe.Emotion.value_counts().to_dict()\n",
        "    def sort_key(item):\n",
        "        return self.vectorizer.emotion_vocab.lookup_token(item[0])\n",
        "    sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "    frequencies = [count for _, count in sorted_counts]\n",
        "    self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "\n",
        "  def __getitem__(self,index) :\n",
        "      row = self.dataframe.iloc[index]\n",
        "      vectorized_text,length_text = self.vectorizer.vectorize(row.Text, self._max_text_length)\n",
        "      target_index = self.vectorizer.emotion_vocab.lookup_token(row.Emotion)\n",
        "      #we return the vectoized version of text as x, and the index of emotion as y we also return the actual length of the text\n",
        "      return {'x_data': vectorized_text,\n",
        "              'y_target': target_index,\n",
        "              \"x_length\" : length_text\n",
        "              }\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe)"
      ],
      "metadata": {
        "id": "C6GNmvfesQFZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#column gather function gets a set of outputs (of RNN cell) and returns the one after t = seeing last token in the text\n",
        "def column_gather(y_out, x_lengths):\n",
        "\n",
        "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
        "\n",
        "    out = []\n",
        "    for batch_index, column_index in enumerate(x_lengths):\n",
        "        out.append(y_out[batch_index, column_index])\n",
        "    return torch.stack(out)\n",
        "\n",
        "#main job of classifier class : it inherits from the Module class in pytorch and implements a forward function, it builds the structure of the\n",
        "#neural network model and is being used for doing forward pass in training process\n",
        "class EmotionClassifier(nn.Module):\n",
        "  def __init__(self,num_classes, text_vocab_size,embedding_size,rnn_hidden_size,padding_idx=0,dropout_p=0.5):\n",
        "    super(EmotionClassifier, self).__init__()\n",
        "    # I created an embedding layer to convert each token to a an ambedded vector. Embedded vecores are being created and tuned during the\n",
        "    # training process. embedding_dim is an arbitary size that we want to have for each embedded token.\n",
        "    self.char_emb = nn.Embedding(num_embeddings=text_vocab_size,\n",
        "                                     embedding_dim=embedding_size,\n",
        "                                     padding_idx=padding_idx)\n",
        "    #GRU is a sequential nerural network layer which generates outputs by using a sequense of inputes and its hidden layer. at each time step \n",
        "    #(seeing one token), it also updates its hidden layer. hidden_size is an arbitary output size for GRU layer\n",
        "    self.rnn = nn.GRU(input_size=embedding_size,\n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          batch_first=True)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                      out_features=rnn_hidden_size)\n",
        "    self.fc2 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                      out_features=num_classes)\n",
        "  \n",
        "  def forward(self,x_in,apply_softmax=False,x_lengths=None):\n",
        "        # we tranfer indexes to embedded vectores\n",
        "        x_embedded = self.char_emb(x_in)\n",
        "        # we use our rnn layer and get a sequence of outputs, each one corresponding to one time step\n",
        "        y_out, _ = self.rnn(x_embedded)\n",
        "\n",
        "        #if x_lengths is providede we select the output of rnn cell which was provided after seeing the last token in the test\n",
        "        if x_lengths is not None:\n",
        "            y_out = column_gather(y_out, x_lengths)\n",
        "        #otherwise we select the last ouput (real tokens + mask tokens)\n",
        "        else:\n",
        "            y_out = y_out[:, -1, :]\n",
        "\n",
        "        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))\n",
        "        y_out = self.fc2(F.dropout(y_out, 0.5))\n",
        "        #soft mask is better to be used just for testing and not during training proceess (makes some mathmatical difficulities with our \n",
        "        #cross entropy loss function)\n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "\n",
        "        return y_out\n"
      ],
      "metadata": {
        "id": "nyY4eBxrDA9-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing and Tranining"
      ],
      "metadata": {
        "id": "jaJhgigEIiIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed_everywhere(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "batch_size=128\n",
        "learning_rate=0.001\n",
        "num_epochs=10\n",
        "seed=1337\n",
        "embedding_size=100\n",
        "rnn_hidden_size=64"
      ],
      "metadata": {
        "id": "KhJgSfROIsLX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not torch.cuda.is_available():\n",
        "    cuda = False\n",
        "    device = torch.device(\"cpu\")\n",
        "else :\n",
        "    cuda = True\n",
        "    device = torch.device(\"cuda\")\n",
        "print(\"Using CUDA: {}, device : {}\".format(cuda,device))\n",
        "set_seed_everywhere(seed,cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9593aPtoI2VF",
        "outputId": "88ad850a-1a0b-4f46-e7df-cf63950ea6dc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA: False, device : cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading dataset and creating vectorizer"
      ],
      "metadata": {
        "id": "Go-JWwBxKVzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we should build the vocabularies and the vectorizer just using our trainig data\n",
        "emotion_vacob = Vocabulary(train_data.Emotion,add_unk = False,add_mask = False)\n",
        "text_vocab = Vocabulary(train_data.Text,add_unk = True) \n",
        "vectorizer = Vectorizer(text_vocab,emotion_vacob)\n",
        "#making test and train datastes\n",
        "train_dataset = Dataset(train_data,vectorizer)\n",
        "test_dataset = Dataset(test_data,vectorizer)\n",
        "classifier = EmotionClassifier(text_vocab_size=len(text_vocab),num_classes=len(emotion_vacob),embedding_size=embedding_size,rnn_hidden_size=rnn_hidden_size)"
      ],
      "metadata": {
        "id": "ieyfjU-VKWeP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(y_pred, y_target):\n",
        "    _, y_pred_indices = y_pred.max(dim=1)\n",
        "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "    return n_correct / len(y_pred_indices) * 100"
      ],
      "metadata": {
        "id": "sLOfWGTromru"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.CrossEntropyLoss(train_dataset.class_weights)\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                           mode='min', factor=0.5,\n",
        "                                           patience=1)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n"
      ],
      "metadata": {
        "id": "SoKNxBQkEehs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss= []\n",
        "train_acc = []\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0.0\n",
        "  running_acc = 0.0\n",
        "  classifier.train()\n",
        "  for batch_index, batch_dict in enumerate(train_dataloader):\n",
        "    #zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #compute the output\n",
        "    y_pred = classifier(x_in=batch_dict['x_data'], \n",
        "                        x_lengths=batch_dict['x_length'])\n",
        "\n",
        "    #compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "\n",
        "    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "\n",
        "    #use loss to produce gradients\n",
        "    loss.backward()\n",
        "\n",
        "    #use optimizer to take gradient step\n",
        "    optimizer.step()\n",
        "    \n",
        "    # compute the accuracy\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "  train_loss.append(running_loss)\n",
        "  train_acc.append(running_acc)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZcJcOeo0H0c8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_loss)\n",
        "plt.plot(train_acc)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "wQXzGtUyHfC4",
        "outputId": "4772289b-fffe-47e1-d85b-88bce66bfdf1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "running_loss = 0.0\n",
        "running_acc = 0.0\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(test_dataloader):\n",
        "  # compute the output\n",
        "  y_pred =  classifier(batch_dict['x_data'],\n",
        "                        x_lengths=batch_dict['x_length'])\n",
        "  \n",
        "  # compute the loss\n",
        "  loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "  loss_t = loss.item()\n",
        "  running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "  # compute the accuracy\n",
        "  acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "  running_acc += (acc_t - running_acc) / (batch_index + 1)\n"
      ],
      "metadata": {
        "id": "k2pYE00ZmMvW"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"test loss : {}, test accuracy : {}\".format(running_loss,running_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib2jYidsISEn",
        "outputId": "7d1cf757-10cd-48db-eb6a-3b94b251d2b5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss : 1.906140460417821, test accuracy : 64.33293269230768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to use the trained model later, it is important to save the model and vocabularies we built from training data"
      ],
      "metadata": {
        "id": "zutw4LtkKRTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "vocabs = {'emotion_vocab': emotion_vacob.token_to_index, 'text_vocab': text_vocab.token_to_index}\n",
        "with open(\"drive/My Drive/Data/ISEAR/vocabs.json\", \"w\") as fp:\n",
        "  json.dump(vocabs, fp)\n",
        "\n",
        "torch.save(classifier,\"drive/My Drive/Data/ISEAR/model1\")"
      ],
      "metadata": {
        "id": "Y5pZm3FsKjZA"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}