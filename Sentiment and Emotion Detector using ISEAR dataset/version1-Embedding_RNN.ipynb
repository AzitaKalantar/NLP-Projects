### Building an Emotion Classifier Model
Extracting Data
from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
train_data = pd.read_csv('drive/My Drive/Data/ISEAR/data_train.csv')
test_data = pd.read_csv('drive/My Drive/Data/ISEAR/data_test.csv')
train_data.columns = ["Emotion","Text"]
test_data.columns = ["Emotion","Text"]
print(train_data.head())
print(test_data.head())
Data Cleaning and Preprocessing
from nltk.corpus import stopwords
from textblob import TextBlob
from nltk.stem import PorterStemmer
def preprocess_data(data):
  #make the text lower case
  data['Text'] = data['Text'].apply(lambda a: " ".join(a.lower() for a in a.split()))
  #remove non-word characters (^\w) or white space characters (\s)
  data['Text'] = data['Text'].apply(lambda a: " ".join(a.replace('[^\w\s]','') for a in a.split()))
  #remove stop words
  stop = stopwords.words('english')
  data['Text'] = data['Text'].apply(lambda a: " ".join(a for a in a.split() if a not in stop))
  #correct spelling
  data['Text'] = data['Text'].apply(lambda a: str(TextBlob(a).correct()))
  #do stemming
  st = PorterStemmer()
  data['Text'] =  data['Text'].apply(lambda a: " ".join([st.stem(word) for word in a.split()]))
  return data


train_data = preprocess_data(train_data)
test_data = preprocess_data(test_data)
print(train_data.head())
print(test_data.head())
train_data.to_csv("drive/My Drive/Data/ISEAR/pre_processsed_data_train.csv")
test_data.to_csv("drive/My Drive/Data/ISEAR/pre_processsed_data_test.csv")
import pandas as pd
train_data = pd.read_csv('drive/My Drive/Data/ISEAR/pre_processsed_data_train.csv')
test_data = pd.read_csv('drive/My Drive/Data/ISEAR/pre_processsed_data_test.csv')
Data Sience :

Build vocabulary, vectorize and Data set and classifier classes
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import torch.nn as nn
from torch.nn import functional as F
import torch.optim as optim
#the main vocabulary class job : creates a token to index and a index to token dictionary. we can access to one of each, given the other
class Vocabulary(object):
  def __init__(self,data=None,add_unk = True,unk_token = "<UNK>",add_mask = True,mask_token="<MASK>"):
    self.token_to_index = {}
    self.index_to_token = {}
    #mask and unk tokens are optional depending on the vocabulary and the problem
    self.add_unk = add_unk
    if add_unk :
      self.unk_token = unk_token
      self.add_token(unk_token)
      self.unk_index = self.token_to_index[unk_token]
    if add_mask:
      self.add_token(mask_token)
      self.mask_index = self.token_to_index[mask_token]
    if data != None :
      for row in data :
        for token in row.split():
          self.add_token(token)

  #this function gets one token and add it to the dictionary by updating both token_to_index and index_to_token      
  def add_token(self,token):
    if token not in self.token_to_index :
      next_index_in_vocab = len(self.token_to_index)
      self.token_to_index[token] = next_index_in_vocab
      self.index_to_token[next_index_in_vocab] = token
  # this function search for a token and returns its corresponding token, if the token is not in the vocabulary and vocabulary supports 
  # unk tokens it returns the index of unk token, otherwise it raise an error 
  def lookup_token(self,token):
    if self.add_unk:
      return self.token_to_index.get(token,self.unk_index)
    else :
      return self.token_to_index[token]
  # this function search for a index and returns its corresponding token
  def lookup_index(self,index):
    return self.index_to_token[index]

  # returns the legth of the vocabulary
  def __len__(self):
    return len(self.token_to_index)

  def use_previous_token_to_index(self,token_to_index):
    self._token_to_idx = token_to_index
    self.index_to_token = {idx: token for token, idx in self.token_to_index.items()}
#the main job of Vectorizer class: it is responsible for converting a text (sequence of tokens) to a vectorized version of it (sequence of indexes)
#so it can be used by neural network layers

class Vectorizer(object):
  def __init__(self,text_vocab,emotion_vocab):
    self.text_vocab = text_vocab
    self.emotion_vocab = emotion_vocab
  #vector_length is usually the lentgh of the maximum text
  #although we have textes with different lengthes but we neet  put them in a fixed-size vector and fill the remaining of the vector with mask
  #tokens. I also return the actual length of each text
  def vectorize(self,text,vector_length):
    indices = [self.text_vocab.lookup_token(token) for token in text.split(' ')]
    out_vector = np.zeros(vector_length, dtype=np.int64)
    out_vector[:len(indices)] = indices
    out_vector[len(indices):] = self.text_vocab.mask_index
    return out_vector,len(indices)

#The main job of Dataset class : Dataset class inherits from Dataset class in pytorch and implements two essential funtions of it, __getitem()__
#and __len__(), this class in being used for getting the dataset rows (vectorized version) during training and testing 
class Dataset(Dataset):
  def __init__(self,dataframe,vectorizer):
    self.dataframe = dataframe
    self.vectorizer = vectorizer
    measure_len = lambda text: len(text.split(" "))
    self._max_text_length = max(map(measure_len, dataframe.Text))

    # Class weights
    class_counts = self.dataframe.Emotion.value_counts().to_dict()
    def sort_key(item):
        return self.vectorizer.emotion_vocab.lookup_token(item[0])
    sorted_counts = sorted(class_counts.items(), key=sort_key)
    frequencies = [count for _, count in sorted_counts]
    self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)

  def __getitem__(self,index) :
      row = self.dataframe.iloc[index]
      vectorized_text,length_text = self.vectorizer.vectorize(row.Text, self._max_text_length)
      target_index = self.vectorizer.emotion_vocab.lookup_token(row.Emotion)
      #we return the vectoized version of text as x, and the index of emotion as y we also return the actual length of the text
      return {'x_data': vectorized_text,
              'y_target': target_index,
              "x_length" : length_text
              }
  def __len__(self):
    return len(self.dataframe)
#column gather function gets a set of outputs (of RNN cell) and returns the one after t = seeing last token in the text
def column_gather(y_out, x_lengths):

    x_lengths = x_lengths.long().detach().cpu().numpy() - 1

    out = []
    for batch_index, column_index in enumerate(x_lengths):
        out.append(y_out[batch_index, column_index])
    return torch.stack(out)

#main job of classifier class : it inherits from the Module class in pytorch and implements a forward function, it builds the structure of the
#neural network model and is being used for doing forward pass in training process
class EmotionClassifier(nn.Module):
  def __init__(self,num_classes, text_vocab_size,embedding_size,rnn_hidden_size,padding_idx=0,dropout_p=0.5):
    super(EmotionClassifier, self).__init__()
    # I created an embedding layer to convert each token to a an ambedded vector. Embedded vecores are being created and tuned during the
    # training process. embedding_dim is an arbitary size that we want to have for each embedded token.
    self.char_emb = nn.Embedding(num_embeddings=text_vocab_size,
                                     embedding_dim=embedding_size,
                                     padding_idx=padding_idx)
    #GRU is a sequential nerural network layer which generates outputs by using a sequense of inputes and its hidden layer. at each time step 
    #(seeing one token), it also updates its hidden layer. hidden_size is an arbitary output size for GRU layer
    self.rnn = nn.GRU(input_size=embedding_size,
                          hidden_size=rnn_hidden_size,
                          batch_first=True)

    self.fc1 = nn.Linear(in_features=rnn_hidden_size,
                      out_features=rnn_hidden_size)
    self.fc2 = nn.Linear(in_features=rnn_hidden_size,
                      out_features=num_classes)
  
  def forward(self,x_in,apply_softmax=False,x_lengths=None):
        # we tranfer indexes to embedded vectores
        x_embedded = self.char_emb(x_in)
        # we use our rnn layer and get a sequence of outputs, each one corresponding to one time step
        y_out, _ = self.rnn(x_embedded)

        #if x_lengths is providede we select the output of rnn cell which was provided after seeing the last token in the test
        if x_lengths is not None:
            y_out = column_gather(y_out, x_lengths)
        #otherwise we select the last ouput (real tokens + mask tokens)
        else:
            y_out = y_out[:, -1, :]

        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))
        y_out = self.fc2(F.dropout(y_out, 0.5))
        #soft mask is better to be used just for testing and not during training proceess (makes some mathmatical difficulities with our 
        #cross entropy loss function)
        if apply_softmax:
            y_out = F.softmax(y_out, dim=1)

        return y_out

Initializing and Tranining
def set_seed_everywhere(seed, cuda):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if cuda:
        torch.cuda.manual_seed_all(seed)

batch_size=128
learning_rate=0.001
num_epochs=10
seed=1337
embedding_size=100
rnn_hidden_size=64
if not torch.cuda.is_available():
    cuda = False
    device = torch.device("cpu")
else :
    cuda = True
    device = torch.device("cuda")
print("Using CUDA: {}, device : {}".format(cuda,device))
set_seed_everywhere(seed,cuda)
Loading dataset and creating vectorizer
# we should build the vocabularies and the vectorizer just using our trainig data
emotion_vacob = Vocabulary(train_data.Emotion,add_unk = False,add_mask = False)
text_vocab = Vocabulary(train_data.Text,add_unk = True) 
vectorizer = Vectorizer(text_vocab,emotion_vacob)
#making test and train datastes
train_dataset = Dataset(train_data,vectorizer)
test_dataset = Dataset(test_data,vectorizer)
classifier = EmotionClassifier(text_vocab_size=len(text_vocab),num_classes=len(emotion_vacob),embedding_size=embedding_size,rnn_hidden_size=rnn_hidden_size)
def compute_accuracy(y_pred, y_target):
    _, y_pred_indices = y_pred.max(dim=1)
    n_correct = torch.eq(y_pred_indices, y_target).sum().item()
    return n_correct / len(y_pred_indices) * 100
loss_func = nn.CrossEntropyLoss(train_dataset.class_weights)
optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,
                                           mode='min', factor=0.5,
                                           patience=1)

train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True,drop_last=True)

train_loss= []
train_acc = []
for epoch in range(num_epochs):
  running_loss = 0.0
  running_acc = 0.0
  classifier.train()
  for batch_index, batch_dict in enumerate(train_dataloader):
    #zero the gradients
    optimizer.zero_grad()

    #compute the output
    y_pred = classifier(x_in=batch_dict['x_data'], 
                        x_lengths=batch_dict['x_length'])

    #compute the loss
    loss = loss_func(y_pred, batch_dict['y_target'])

    running_loss += (loss.item() - running_loss) / (batch_index + 1)

    #use loss to produce gradients
    loss.backward()

    #use optimizer to take gradient step
    optimizer.step()
    
    # compute the accuracy
    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])
    running_acc += (acc_t - running_acc) / (batch_index + 1)

  train_loss.append(running_loss)
  train_acc.append(running_acc)




import matplotlib.pyplot as plt
plt.plot(train_loss)
plt.plot(train_acc)
plt.show()
running_loss = 0.0
running_acc = 0.0
classifier.eval()

for batch_index, batch_dict in enumerate(test_dataloader):
  # compute the output
  y_pred =  classifier(batch_dict['x_data'],
                        x_lengths=batch_dict['x_length'])
  
  # compute the loss
  loss = loss_func(y_pred, batch_dict['y_target'])
  loss_t = loss.item()
  running_loss += (loss_t - running_loss) / (batch_index + 1)

  # compute the accuracy
  acc_t = compute_accuracy(y_pred, batch_dict['y_target'])
  running_acc += (acc_t - running_acc) / (batch_index + 1)

print("test loss : {}, test accuracy : {}".format(running_loss,running_acc))
To be able to use the trained model later, it is important to save the model and vocabularies we built from training data
import json

vocabs = {'emotion_vocab': emotion_vacob.token_to_index, 'text_vocab': text_vocab.token_to_index}
with open("drive/My Drive/Data/ISEAR/vocabs.json", "w") as fp:
  json.dump(vocabs, fp)

torch.save(classifier,"drive/My Drive/Data/ISEAR/model1")
