{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPbQWRG71FtQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('C:\\\\Users\\\\parvi\\\\Documents\\\\toturials\\\\NLP\\\\NLP Projects\\\\NLP-Projects\\\\data\\\\Enron\\\\preprocessed_emails.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[\"body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T7HV5YxmCOJO",
    "outputId": "1e5fa7da-9587-4783-98d5-dbd3234d62aa"
   },
   "outputs": [],
   "source": [
    "#the Enron emails datase is originally 880 MB and my RAM didn't support this much data, so I had to reduce the size of \n",
    "#data\n",
    "reduced_data = data.sample(frac=0.001)\n",
    "reduced_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYAIpUplCL5y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzeYZ45hCJFY"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(reduced_data, test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "id": "4yu3yZh-TXVL",
    "outputId": "bdac1272-d2c4-49b0-bdca-a32af8857307"
   },
   "outputs": [],
   "source": [
    "train.to_csv('C:\\\\Users\\\\parvi\\\\Documents\\\\toturials\\\\NLP\\\\NLP Projects\\\\NLP-Projects\\\\data\\\\Enron\\\\train.csv',index=False)\n",
    "test.to_csv('C:\\\\Users\\\\parvi\\\\Documents\\\\toturials\\\\NLP\\\\NLP Projects\\\\NLP-Projects\\\\data\\\\Enron\\\\test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdoreSpSTddU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('C:\\\\Users\\\\parvi\\\\Documents\\\\toturials\\\\NLP\\\\NLP Projects\\\\NLP-Projects\\\\data\\\\Enron\\\\train.csv')\n",
    "test = pd.read_csv('C:\\\\Users\\\\parvi\\\\Documents\\\\toturials\\\\NLP\\\\NLP Projects\\\\NLP-Projects\\\\data\\\\Enron\\\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOMf3tjwCvH0",
    "outputId": "e22ad933-7e24-4258-920b-2e3b7cb0a804"
   },
   "outputs": [],
   "source": [
    "train =train.squeeze()\n",
    "test = test.squeeze()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8j1alEkKchCe"
   },
   "outputs": [],
   "source": [
    "train = train.to_numpy()\n",
    "test = test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ehkKsoaaC8gT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "gi18nS5vDVuq"
   },
   "outputs": [],
   "source": [
    "#the main vocabulary class job : creates a token to index and a index to token dictionary. we can access to one of each, given the other\n",
    "class Vocabulary(object):\n",
    "  def __init__(self,data=None,unk_token = \"<UNK>\",mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",end_seq_token=\"<END>\"):\n",
    "    self.token_to_index = {}\n",
    "    self.index_to_token = {}\n",
    "\n",
    "    self.unk_token = unk_token\n",
    "    self.add_token(unk_token)\n",
    "    self.unk_index = self.token_to_index[unk_token]\n",
    "\n",
    "    self.mask_token = mask_token\n",
    "    self.add_token(mask_token)\n",
    "    self.mask_index = self.token_to_index[mask_token]\n",
    "\n",
    "    self.begin_seq_token = begin_seq_token\n",
    "    self.add_token(begin_seq_token)\n",
    "    self.begin_seq_index = self.token_to_index[begin_seq_token]\n",
    "\n",
    "    self.end_seq_token = end_seq_token\n",
    "    self.add_token(end_seq_token)\n",
    "    self.end_seq_index = self.token_to_index[end_seq_token]\n",
    "\n",
    "    if data is not None :\n",
    "      for row in data :\n",
    "        for token in word_tokenize(row):\n",
    "          self.add_token(token)\n",
    "\n",
    "  #this function gets one token and add it to the dictionary by updating both token_to_index and index_to_token      \n",
    "  def add_token(self,token):\n",
    "    if token not in self.token_to_index :\n",
    "      next_index_in_vocab = len(self.token_to_index)\n",
    "      self.token_to_index[token] = next_index_in_vocab\n",
    "      self.index_to_token[next_index_in_vocab] = token\n",
    "\n",
    "  # this function search for a token and returns its corresponding token, if the token is not in the vocabulary and vocabulary supports \n",
    "  # unk tokens it returns the index of unk token, otherwise it raise an error \n",
    "  def lookup_token(self,token):\n",
    "\n",
    "    return self.token_to_index.get(token,self.unk_index)\n",
    "\n",
    "  # this function search for a index and returns its corresponding token\n",
    "  def lookup_index(self,index):\n",
    "\n",
    "    if index not in self.index_to_token:\n",
    "        raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "    return self.index_to_token[index]\n",
    "\n",
    "  # returns the legth of the vocabulary\n",
    "  def __len__(self):\n",
    "\n",
    "    return len(self.token_to_index)\n",
    "\n",
    "  def use_previous_token_to_index(self,token_to_index):\n",
    "    self.token_to_index = token_to_index\n",
    "    self.index_to_token = {idx: token for token, idx in self.token_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfRh69Q0GEsV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "1fh_XyKRnG9R"
   },
   "outputs": [],
   "source": [
    "#the main job of Vectorizer class: it is responsible for converting a text (sequence of tokens) to a vectorized version of it (sequence of indexes)\n",
    "#so it can be used by neural network layers\n",
    "\n",
    "class Vectorizer(object):\n",
    "  def __init__(self,text):\n",
    "    self.text_vocab = text\n",
    " \n",
    "  #vector_length is usually the lentgh of the maximum text\n",
    "  #although we have textes with different lengthes but we neet to put them in a fixed-size vector and fill the remaining of the vector with mask\n",
    "  #tokens. \n",
    "  def vectorize(self,text,vector_length):\n",
    "\n",
    "    indices = [self.text_vocab.begin_seq_index] \n",
    "    indices.extend(self.text_vocab.lookup_token(token) for token in word_tokenize(text))\n",
    "    indices.append(self.text_vocab.end_seq_index)\n",
    "\n",
    "    from_vector = np.empty(vector_length, dtype=np.int64)         \n",
    "    from_indices = indices[:-1]\n",
    "    from_vector[:len(from_indices)] = from_indices\n",
    "    from_vector[len(from_indices):] = self.text_vocab.mask_index\n",
    "\n",
    "    to_vector = np.empty(vector_length, dtype=np.int64)\n",
    "    to_indices = indices[1:]\n",
    "    to_vector[:len(to_indices)] = to_indices\n",
    "    to_vector[len(to_indices):] = self.text_vocab.mask_index\n",
    "    \n",
    "    return from_vector, to_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "F-Whf5r3JFf9"
   },
   "outputs": [],
   "source": [
    "#The main job of Dataset class : Dataset class inherits from Dataset class in pytorch and implements two essential funtions of it, __getitem()__\n",
    "#and __len__(), this class in being used for getting the dataset rows (vectorized version) during training and testing \n",
    "class Dataset(Dataset):\n",
    "  def __init__(self,body,vectorizer):\n",
    "    self.body = body\n",
    "    self.vectorizer = vectorizer\n",
    "    measure_len = lambda text: len(word_tokenize(text))\n",
    "    self._max_text_length = max(map(measure_len,body)) + 2\n",
    "\n",
    "\n",
    "\n",
    "  def __getitem__(self,index) :\n",
    "      text = self.body[index]\n",
    "      from_vector, to_vector = self.vectorizer.vectorize(text, self._max_text_length)\n",
    "\n",
    "      #we return the vectoized version of text as x, and the index of emotion as y \n",
    "      return {'x_data': from_vector,\n",
    "              'y_target': to_vector\n",
    "              }\n",
    "  def __len__(self):\n",
    "    return len(self.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "29H8zRwsKrUR"
   },
   "outputs": [],
   "source": [
    "class WordGenerationModel(nn.Module):\n",
    "  def __init__(self, body_vocab_size,embedding_size,rnn_hidden_size,padding_idx=0,dropout_p=0.5):\n",
    "    super(WordGenerationModel, self).__init__()\n",
    "    # I created an embedding layer to convert each token to a an ambedded vector. Embedded vecores are being created and tuned during the\n",
    "    # training process. embedding_dim is an arbitary size that we want to have for each embedded token.\n",
    "    self.word_emb = nn.Embedding(num_embeddings=body_vocab_size,\n",
    "                                     embedding_dim=embedding_size,\n",
    "                                     padding_idx=padding_idx)\n",
    "    #GRU is a sequential nerural network layer which generates outputs by using a sequense of inputes and its hidden layer. at each time step \n",
    "    #(seeing one token), it also updates its hidden layer. hidden_size is an arbitary output size for GRU layer\n",
    "    self.rnn = nn.GRU(input_size=embedding_size,\n",
    "                          hidden_size=rnn_hidden_size,\n",
    "                          batch_first=True)\n",
    "\n",
    "    self.fc1 = nn.Linear(in_features=rnn_hidden_size,\n",
    "                      out_features=body_vocab_size)\n",
    "    self.fc2 = nn.Linear(in_features=rnn_hidden_size,\n",
    "                      out_features=body_vocab_size)\n",
    "  \n",
    "  def forward(self,x_in,apply_softmax=False):\n",
    "        # we tranfer indexes to embedded vectores\n",
    "        x_embedded = self.word_emb(x_in)\n",
    "        # we use our rnn layer and get a sequence of outputs, each one corresponding to one time step\n",
    "        y_out, _ = self.rnn(x_embedded)\n",
    "\n",
    "        batch_size, seq_size, feat_size = y_out.shape\n",
    "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "\n",
    "        y_out = self.fc1(F.dropout(y_out, 0.5))\n",
    "                         \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "            \n",
    "        new_feat_size = y_out.shape[-1]\n",
    "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
    "            \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNwAK4hpZLS2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "pt6Fh-mUbQ36"
   },
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "batch_size=32\n",
    "learning_rate=0.001\n",
    "num_epochs=10\n",
    "seed=1337\n",
    "embedding_size=100\n",
    "rnn_hidden_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhaqJ3nMbtUx",
    "outputId": "089135cc-0578-4570-a63c-908870fa8a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False, device : cpu\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    cuda = False\n",
    "    device = torch.device(\"cpu\")\n",
    "else :\n",
    "    cuda = True\n",
    "    device = torch.device(\"cuda\")\n",
    "print(\"Using CUDA: {}, device : {}\".format(cuda,device))\n",
    "set_seed_everywhere(seed,cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H224NwgDWUmh"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3M_TYhSZc6FF"
   },
   "outputs": [],
   "source": [
    "text_vacob = Vocabulary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6AFFM-ThGx-"
   },
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(text_vacob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCbpA_X1hIKX"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train,vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbl82HOHiRLO"
   },
   "outputs": [],
   "source": [
    "test_dataset = Dataset(test,vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hv3zkhHiUs5"
   },
   "outputs": [],
   "source": [
    "model = WordGenerationModel(body_vocab_size=len(text_vacob),embedding_size=embedding_size,rnn_hidden_size=rnn_hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCHUH2obiyeX"
   },
   "outputs": [],
   "source": [
    "#y_pred's shape is (Batch size,Sequence size,feature size). feature size is a one hot vector with the size of vocabulary and determines\n",
    "#which world in the vocabulary has been selected.\n",
    "#y_true's shape is (Batch size,Sequence size) and in each cell of this matrix we have an integer indicating the index of the target\n",
    "#word in the vocabulary.\n",
    "#cross entropy loss function's input should be with shape : y_pred => matrix , y_true => 1d array. so we need to to convert 3d to 2d and 2d\n",
    "#to 1d to be able to use this loss function\n",
    "\n",
    "def normalize_sizes(y_pred, y_true):\n",
    "\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "#this is the loss which use normalize_size function before cross_entropy\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwjVvWx5i-7C"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmNt9R4pk2gz"
   },
   "outputs": [],
   "source": [
    "# I ran this code for  5 hours with num_epochs = 10\n",
    "model = model.to(device)\n",
    "mask_index = vectorizer.text_vocab.mask_index\n",
    "train_loss= []\n",
    "train_acc = []\n",
    "val_loss= []\n",
    "val_acc = []\n",
    "smalles_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "  running_loss = 0.0\n",
    "  running_acc = 0.0\n",
    "\n",
    "  model.train()\n",
    "  for batch_index, batch_dict in enumerate(train_dataloader):\n",
    "    #zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #compute the output\n",
    "    y_pred = model(x_in=batch_dict['x_data'])\n",
    "\n",
    "    #compute the loss\n",
    "    loss = sequence_loss(y_pred, batch_dict['y_target'],mask_index)\n",
    "\n",
    "    #running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "    #use loss to produce gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #use optimizer to take gradient step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # compute the accuracy\n",
    "    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "    train_loss.append(running_loss)\n",
    "    train_acc.append(running_acc)\n",
    "\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "model.eval()\n",
    "mask_index = vectorizer.text_vocab.mask_index\n",
    "for batch_index, batch_dict in enumerate(test_dataloader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #compute the output\n",
    "        y_pred = model(x_in=batch_dict['x_data'])\n",
    "\n",
    "        #compute the loss\n",
    "        loss = sequence_loss(y_pred, batch_dict['y_target'],mask_index)\n",
    "\n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the vocabulary into a json file\n",
    "import json\n",
    "vocabs = {'text_vocab': text_vacob.token_to_index}\n",
    "with open(\"C:\\\\Users\\\\parvi\\\\Documents\\\\toturials\\\\NLP\\\\NLP Projects\\\\NLP-Projects\\\\data\\\\Enron\\\\vocabs.json\", \"w\") as fp:\n",
    "  json.dump(vocabs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the torch model\n",
    "torch.save(model,\"C:\\\\Users\\\\parvi\\\\Documents\\\\toturials\\\\NLP\\\\NLP Projects\\\\NLP-Projects\\\\data\\\\Enron\\\\model2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the vicanulary\n",
    "import json\n",
    "with open(\"C:\\\\Users\\\\parvi\\\\Documents\\\\toturials\\\\NLP\\\\NLP Projects\\\\NLP-Projects\\\\data\\\\Enron\\\\vocabs.json\",\"r\") as fp:\n",
    "    vocabs = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_index = vocabs[\"text_vocab\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vacob_loaded = Vocabulary()\n",
    "text_vacob_loaded.use_previous_token_to_index(token_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = Vectorizer(text_vacob_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordGenerationModel(\n",
       "  (word_emb): Embedding(15754, 100, padding_idx=0)\n",
       "  (rnn): GRU(100, 32, batch_first=True)\n",
       "  (fc1): Linear(in_features=32, out_features=15754, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=15754, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"C:\\\\Users\\\\parvi\\\\Documents\\\\toturials\\\\NLP\\\\NLP Projects\\\\NLP-Projects\\\\data\\\\Enron\\\\model2\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_the_next_n_words(encoded_text ,model, n=5, temperature=1.0):\n",
    "    indices = []\n",
    "    for index in encoded_text:\n",
    "        indices.append(torch.tensor( [index] , dtype=torch.int64).unsqueeze(dim=1))\n",
    "\n",
    "    h_t = None\n",
    "    for index in indices:\n",
    "        x_emb_t = model.word_emb(index)\n",
    "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
    "        \n",
    "    last_word_index = len(indices)- 1    \n",
    "    for time_step in range(n):\n",
    "        #print(n)\n",
    "        x_t = indices[last_word_index + time_step]\n",
    "        x_emb_t = model.word_emb(x_t)\n",
    "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
    "        prediction_vector = model.fc1(rnn_out_t.squeeze(dim=1))\n",
    "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
    "        #_, predicted = torch.max(probability_vector, 1)\n",
    "        #indices.append(torch.tensor( [predicted] , dtype=torch.int64).unsqueeze(dim=1))\n",
    "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
    "    #print(indices)\n",
    "    indices = torch.stack(indices).squeeze()\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def decode_samples(sampled_indices, vectorizer):\n",
    "\n",
    "\n",
    "    vocab = vectorizer.text_vocab\n",
    "\n",
    "    sentence = []\n",
    "    for time_step in range(sampled_indices.shape[0]):\n",
    "        sample_item = sampled_indices[time_step].item()\n",
    "        if sample_item == vocab.begin_seq_index:\n",
    "            continue\n",
    "        elif sample_item == vocab.end_seq_index:\n",
    "            break\n",
    "        else:\n",
    "            sentence.append(vocab.lookup_index(sample_item))\n",
    "\n",
    "    return sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_samples(text,vectorizer):\n",
    "    indices = [vectorizer.text_vocab.begin_seq_index] \n",
    "    indices.extend(vectorizer.text_vocab.lookup_token(token) for token in word_tokenize(text))\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please see if Scott will be able to meet with some Howard finance <UNK> on January 25 , 2001 . The latest <UNK> in the evening will work if Alternatively with'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Please see if Scott will be able to meet with some Howard finance professor's\n",
    "on January 25, 2001.  The latest appointment in the evening will work if \"\"\"\n",
    "nunmber_guess_words = 2\n",
    "encoded_text = encode_samples(text,vectorizer2)\n",
    "encoded_text_extended = guess_the_next_n_words(encoded_text=encoded_text,model = model,n=nunmber_guess_words)\n",
    "extended_text = \" \".join(decode_samples(encoded_text_extended,vectorizer2))\n",
    "extended_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Congratulations on your promotion CEO rebuild'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Congratulations on your promotion \"\"\"\n",
    "nunmber_guess_words = 2\n",
    "encoded_text = encode_samples(text,vectorizer2)\n",
    "encoded_text_extended = guess_the_next_n_words(encoded_text=encoded_text,model = model,n=nunmber_guess_words)\n",
    "extended_text = \" \".join(decode_samples(encoded_text_extended,vectorizer2))\n",
    "extended_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Could you please <UNK> me an electronic version of your resume ? I will need this to pass to the HR <UNK> people in the Research Group that will be There .'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Could you please furnish me an electronic version of your resume?  I will\n",
    "need this to pass to the HR Dept.The people in the Research Group that will be \"\"\"\n",
    "nunmber_guess_words = 2\n",
    "encoded_text = encode_samples(text,vectorizer2)\n",
    "encoded_text_extended = guess_the_next_n_words(encoded_text=encoded_text,model = model,n=nunmber_guess_words)\n",
    "extended_text = \" \".join(decode_samples(encoded_text_extended,vectorizer2))\n",
    "extended_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
